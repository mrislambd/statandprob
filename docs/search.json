[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic Statistics and Probability",
    "section": "",
    "text": "Correlation, Bivariate, and Regression Analysis\n\n\n\nRafiq Islam\n\n\nWednesday, December 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHello\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html#binary-search",
    "href": "index.html#binary-search",
    "title": "Basic Statistics and Probability",
    "section": "",
    "text": "Problem 1: Leetcode 69: Sqrt(x)\n\n\n\n\n\nGiven a non-negative integer, \\(x\\), return the square root of \\(x\\) rounded down to the nearest integer. The returned integer should be non-negative as well.\nYou may not use any built-in exponent function. For example, x**0.5 in python.\nExample:\nInput: x=4\nOutput: 2  \n\nInput: x=8\nOutput: 2\nExplanation: Square root of 4 is 2 and square root of 8 is 2.8284. But we need to round down to any fraction. Therefore, the square root of 8 is also 2.\nSolution:\nThe square root of any number \\(x\\ge 0\\) is less than or equal to \\(x\\). The brute force solution to this would be \\(\\mathcal{O}(\\sqrt{n})\\). Because, say \\(x=8\\), then\nfor \\(i=1\\) to 8: \\[\\begin{align*}\n  1^2 & = 1 &lt;8 \\\\\n  2^2 & = 4 &lt;8\\\\\n  3^2 & = 9 &gt;8\n\\end{align*}\\]\n\nIn contrast, if we explore binary search then the time complexity reduces to \\(\\mathcal{O}(\\log{n})\\). Say the square root is \\(s\\) which is the middle value in the range of 1 to \\(x\\). Then if \\(s^2&gt;x\\), we search for the root in the left half. Otherwise, if \\(s^2&lt;x\\) then we search the right side. However, when \\(s^2&lt;x\\), then \\(s\\) is a possible candidate for the square root.\nAlgorithm:\n\nset left value \\(l= 0\\), right value \\(r= x\\)\n\nCompute the middle value \\(m=l+(r-l)/2\\)\n\nIf \\(m^2 &gt; x\\) then search the left side: set \\(r=m-1\\)\n\nIf \\(m^2 &lt; x\\) then search the right side: set \\(l=m+1\\)\n\n\ndef square_root(x):\n  l, r = 0, x \n  sq = 0\n  while l&lt;=r:\n    m = l + (r-l)//2 \n    if m**2 &gt; x:\n      r= m-1\n    elif m**2 &lt; x:\n      l = m+1 \n      sq = m\n    else:\n      return m  \n  return sq \n      \nprint(square_root(6))\n\n2"
  },
  {
    "objectID": "index.html#array",
    "href": "index.html#array",
    "title": "Basic Statistics and Probability",
    "section": "Array",
    "text": "Array\n\n\n\n\n\n\nProblem 1: Intersection of two sets\n\n\n\n\n\nSay, we are given two sets \\(A=[2,3,5,6,8]\\) and \\(B=[4,6,8]\\). We want to find the intersection of the elements in these sets.\ndef intersection_of_two_sets(A,B):\n  set_a = set(A)\n  return [b for b in B if b in set_a]\n\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n[6, 8]\n\n\n\n\n\n\n\n\n\nProblem 2: Histogram from a given array and bin number\n\n\n\n\n\nSay, we are given an array \\(A=[1,2,2,3,3,3,4,4,4,4,5,5,5,5,5]\\) and number of bins. We want to\ndef generate_histogram(A, num_bins):\n  min_value = min(A)\n  max_value = max(A)\n  bin_width = (max_value-min_value)/num_bins"
  },
  {
    "objectID": "index.html#string",
    "href": "index.html#string",
    "title": "Basic Statistics and Probability",
    "section": "String",
    "text": "String\n\n\n\n\n\n\nProblem 1: Leetcode 242: Valid Anagram\n\n\n\n\n\nGiven two strings s and t, return true if t is an anagram of s, and false otherwise\nExample: Input: s=\"anagram\", t=\"nagaram\" Output: true\nSince the word anagram has 3 a’s, 1 n, 1 g, 1 r, and 1 m and nagaram has exactly the same number of the same alphabets, therefore they are anagram of each other.\nExample: Input: s=\"rat\", t=\"cat\" Output: false\nSince the word rat has 1 r, 1 a, and 1 t but cat has 2 elements same as rat but one element different. Therefore the answer is false.\nNote that, they both have the same length.\nConstraints:\n\n\\(1\\le s\\).length, \\(t\\).length \\(\\le 5\\times 10^4\\)\n\ns and t consists of lowercase English letters\n\nSolution:\nWe can use hasmap to solve this problem. Basically, we will create two hasmaps for two words and match the keys and values of the hasmaps. If they are equal then it’s an anagram, otherwise not.\n\ndef isAnagram(s,t):\n  if len(s) != len(t):\n    return False \n  \n  hash_s, hash_t = {}, {}\n  for i in range(len(s)):\n    hash_s[s[i]] = 1 + hash_s.get(s[i],0)  # get function collects the key and values.\n    hash_t[t[i]] = 1 + hash_t.get(t[i],0)  # if there's no key, 0 is the default value\n  \n  for c in hash_s:\n    if hash_s[c] != hash_t.get(c,0):       # Here, get function ensures there is no \n      return False                         # key error\n  return True\n\ns = \"anagram\"\nt = \"nagaram\"\n\nprint(isAnagram(s,t))\n\nu = \"rat\"\n\nprint(isAnagram(s,u))\n\nTrue\nFalse\n\n\nTime and Space Complexity:\nTime complexity \\(\\mathcal{O}(m+n)\\) where \\(m\\) and \\(n\\) are the length of s and t and memory complexity is the same \\(\\mathcal{O}(m+n)\\)\nOptimization: Can we solve the problem in \\(\\mathcal{O}(1)\\)? If we assume sort doesn’t require extra space, then\n\ndef isAnagram(s,t):\n  return sorted(s)== sorted(t)\n\ns= \"anagram\"\nt= \"nagaram\"\n\nprint(isAnagram(s,t))\n\nTrue"
  },
  {
    "objectID": "index.html#reference",
    "href": "index.html#reference",
    "title": "Basic Statistics and Probability",
    "section": "Reference",
    "text": "Reference\nAll my solutions here are based on the solutions found from NeetCode."
  },
  {
    "objectID": "posts/lln/index.html",
    "href": "posts/lln/index.html",
    "title": "Hello",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "posts/corrandreg/index.html",
    "href": "posts/corrandreg/index.html",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "posts/corrandreg/index.html#introduction",
    "href": "posts/corrandreg/index.html#introduction",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "posts/corrandreg/index.html#correlation-analysis",
    "href": "posts/corrandreg/index.html#correlation-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nTo better explain, we will use the following hypothetical stock data of 10 companies with stock price and their corresponding proportion in the portfolio.\n\n\nCode\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\n\ndf.set_index('Stock', inplace=True)\n\ndf.T\n\n\n\n\n\n\n\n\nStock\nApple\nCiti\nMS\nWF\nGS\nGoogle\nAmazon\nTesla\nToyota\nSPY\n\n\n\n\nStockPrice\n2.11\n2.42\n2.52\n3.21\n3.62\n3.86\n4.13\n4.27\n4.51\n5.01\n\n\nPortfolio\n2.12\n2.16\n2.51\n2.65\n3.62\n3.15\n4.32\n3.31\n4.18\n4.45\n\n\n\n\n\n\n\nThe scatterplot of the data looks like this\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport matplotlib.pyplot as plt\nplt.scatter(df.StockPrice, df.Portfolio, color='red')\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the graph that there appears to be a linear relationship between the \\(x\\) and \\(y\\) values in this case. To find the relationship mathematically we define the followings\n\\[\\begin{align*}\nS_{xx}& = \\sum (x_i-\\bar{x})^2 = \\sum (x_i^2-2x_i\\bar{x}+\\bar{x}^2)\\\\\n& = \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2= \\sum x_i^2 - 2\\bar{x} n \\bar{x} + n \\bar{x}^2 = \\sum x_i ^2 - n \\bar{x}^2\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\nS_{yy}& = \\sum (y_i-\\bar{y})^2=\\sum y_i ^2 - n \\bar{y}^2\\\\\nS_{xy} & = \\sum (x_i-\\bar{x})^2 \\sum (y_i-\\bar{y})^2 = \\sum x_iy_i -n \\bar{xy}\n\\end{align*}\\]\nThe sample correlation coefficient \\(r\\) is then given as\n\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} = \\frac{\\sum x_i ^2 - n \\bar{x}^2}{\\sqrt{\\left(\\sum x_i ^2 - n \\bar{x}^2\\right)\\left(\\sum y_i ^2 - n \\bar{y}^2\\right)}}\n\\]\nYou may have seen a different formula to calculate this quantity which often looks a bit different\n\\[\n\\rho = Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{var(X)var(Y)}}\n\\]\nThe sample correlation coefficient, \\(r\\), is an estimator of the population correlation coefficient, \\(\\rho\\), in the same way as \\(\\bar{X}\\) is an estimator of \\(\\mu\\) or \\(S^2\\) is an estimator of \\(\\sigma^2\\) . Now the question is what does this \\(r\\) values mean?\n\n\n\n\n\n\n\nValue\nMeaning\n\n\n\n\n\\(r=1\\)\nThe two variables move together in the same direction in a perfect linear relationship.\n\n\n\\(0 &lt; r &lt; 1\\)\nThe two variables tend to move together in the same direction but there is NOT a direct relationship.\n\n\n\\(r= 0\\)\nThe two variables can move in either direction and show no linear relationship.\n\n\n\\(-1 &lt; r &lt; 0\\)\nThe two variables tend to move together in opposite directions but there is not a direct relationship.\n\n\n\\(r =-1\\)\nThe two variables move together in opposite directions in a perfect linear relationship.\n\n\n\nLet’s calculate the correlation of our stock data.\n\n\nCode\nimport math\nx = df.StockPrice.values\ny = df.Portfolio.values\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 32.47\nSₓₓ: 8.53\nSᵧᵧ: 6.97\nSₓᵧ: 7.13\n \nr : 0.92"
  },
  {
    "objectID": "posts/corrandreg/index.html#bivariate-analysis",
    "href": "posts/corrandreg/index.html#bivariate-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\nThe joint probability density function for \\(X\\) and \\(Y\\) in the bivariate normal distribution is given by:\n\\[\nf_{X,Y}(x, y) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}}\n\\exp\\left( -\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho\\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] \\right)\n\\]\n\nWhen \\(|\\rho| = 1\\), the denominator \\(\\sqrt{1-\\rho^2}\\) in the PDF becomes zero, which might appear problematic. However, what happens in this case is that the joint distribution degenerates into a one-dimensional structure (a line) rather than being a two-dimensional probability density.\n\nTo see why, consider the quadratic term inside the exponential:\n\\[\nQ = \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho \\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2}\n\\]\nWhen \\(|\\rho| = 1\\), this quadratic expression simplifies, as shown next.\nStart with the simplified \\(Q\\) when \\(|\\rho| = 1\\):\n\\[\\begin{align*}\nQ &= \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)^2 - 2\\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\cdot \\frac{y-\\mu_Y}{\\sigma_Y} \\right) + \\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\\\\\n&=\\left( \\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\n\\end{align*}\\]\nThis is a perfect square because the “cross term” cancels out all independent variability of \\(X\\) and \\(Y\\) when \\(|\\rho| = 1\\).\nFor the quadratic term \\(Q\\) to have any non-zero probability density (since it appears in the exponent of the PDF), it must be equal to zero: \\[\n\\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} = 0\n\\]\nRearranging this equation: \\[\n\\frac{y-\\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nMultiply through by \\(\\sigma_Y\\): \\[\ny - \\mu_Y = \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\]\nThus:\n\\[\\begin{align*}\n\\mathbb{E}(Y| X=x)&= \\mu_Y + \\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X)\\\\\n& = \\mu_Y + \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\end{align*}\\]\nThis is the equation of a straight line in the \\((X, Y)\\)-plane. The slope of the line is \\(\\rho \\frac{\\sigma_Y}{\\sigma_X}\\), and the line passes through the point \\((\\mu_X, \\mu_Y)\\). When \\(|\\rho| = 1\\), all the joint probability mass collapses onto this line, meaning \\(X\\) and \\(Y\\) are perfectly linearly dependent.\n\n\nCode\nimport numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the bivariate normal PDF\ndef bivariate_normal_pdf(x, y, mu_x, mu_y, sigma_x, sigma_y, rho):\n    z = (\n        ((x - mu_x) ** 2) / sigma_x**2\n        - 2 * rho * (x - mu_x) * (y - mu_y) / (sigma_x * sigma_y)\n        + ((y - mu_y) ** 2) / sigma_y**2\n    )\n    denominator = 2 * np.pi * sigma_x * sigma_y * np.sqrt(1 - rho**2)\n    return np.exp(-z / (2 * (1 - rho**2))) / denominator\n\n# Parameters\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Function to plot the bivariate normal distribution and a line for rho = 1 or -1\ndef plot_bivariate_and_line_side_by_side(rho1, rho2):\n    fig = plt.figure(figsize=(8, 4))\n\n    # Plot for the first rho\n    ax1 = fig.add_subplot(121, projection='3d')\n    if abs(rho1) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax1.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho1})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho1)\n        ax1.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax1.set_title(f'Bivariate Normal (ρ = {rho1:.2f})')\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('PDF')\n\n    # Plot for the second rho\n    ax2 = fig.add_subplot(122, projection='3d')\n    if abs(rho2) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax2.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho2})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho2)\n        ax2.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax2.set_title(f'Bivariate Normal (ρ = {rho2:.2f})')\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_zlabel('PDF')\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot examples side by side\nplot_bivariate_and_line_side_by_side(0.5, 1)  # Example with rho = 0.5 and rho = 1\n\n\n\n\n\n\n\n\n\n\n\\(t-\\)Statistic\nUnder the null hypothesis, where \\(H_0: \\rho =0, \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\) has a \\(t-\\) distribution with \\(\\nu=n-2\\) degree of freedom.\n\n\nFisher’s Transformation of \\(r\\)\nIf \\(W = \\frac{1}{2}\\ln{\\frac{1+r}{1-r}}=\\tanh^{-1}r\\), then \\(W\\) has approximately a normal distribution with mean \\(\\frac{1}{2}\\ln{\\frac{1+\\rho}{1-\\rho}}\\) and standard deviation \\(\\frac{1}{\\sqrt{n-3}}\\).\nFor our stock data:\nNull Hypothesis \\(H_0\\): There is no association between stock prices and the portfolio values, i.e., \\(\\rho =0\\)\nAlternative Hypothesis \\(H_1\\): There is some association between the stock price and portfolio values, i.e., \\(\\rho &gt; 0\\)\nIf \\(H_0\\) is true, then the test statistic \\(\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{0.92\\sqrt{8}}{\\sqrt{1-0.92^2}}=6.64\\) has a \\(t_8\\) distribution. The observed value \\(6.64\\) is much greater than the critical value of \\(t_8\\) at \\(0.5\\%\\) level which is \\(3.36\\).\nSo, we reject the null hypothesis \\(H_0\\) at the \\(0.5\\%\\) level and conclude that there is a very strong evidence that \\(\\rho&gt;0\\).\nAlternatively, if we want to use the Fisher’s test:\nIf \\(H_0\\) is true, then the test statistic \\(Z_r=\\tanh^{-1}r=\\tanh^{-1}(0.92)\\) has a \\(N\\left(0,\\frac{1}{7}\\right)\\) distribution.\nThe observed value of this statistic is \\(\\frac{1}{2}\\log{\\frac{1+0.92}{1-0.92}}=1.589\\), which corresponds to a value of \\(\\frac{1.589}{\\sqrt{\\frac{1}{7}}}=4.204\\) on the \\(N(0,1)\\) distribution. This is much greater than \\(3.090\\), the upper \\(0.1\\%\\) point of the standard normal distribution.\nSo, we reject \\(H_0\\) at the \\(0.1\\%\\) level and conclude that there is very strong evidence that \\(\\rho &gt; 0\\) ie that there is a positive linear correlation between the stock price and portfolio value."
  },
  {
    "objectID": "posts/corrandreg/index.html#regression-analysis",
    "href": "posts/corrandreg/index.html#regression-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Regression Analysis",
    "text": "Regression Analysis\nGiven a set of points \\((x_i,y_i)_{i=0}^{n}\\) for a simple linear regression of the form\n\\[\nY_i = \\alpha +\\beta x_i + \\epsilon_i; \\hspace{4mm} i=1,2,\\cdots,n\n\\]\nwith \\(\\mathbb{\\epsilon_i}=0\\) and \\(var[\\epsilon_i]=\\sigma^2\\).\n\nModel Fitting\nWe can estimate the parameters from the method of least squares but that’s not the goal in this case. Fitting the model involves finding \\(\\alpha\\) and \\(\\beta\\) and the estimating the variance \\(\\sigma^2\\).\n\\[\n\\hat{y} = \\hat{\\alpha}+\\hat{\\beta}x\n\\]\nwhere, \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\) and \\(\\hat{\\alpha} = \\bar{y}-\\hat{\\beta}\\bar{x}\\)\n\\(\\hat{\\beta}\\) is the observed value of a statistic \\(\\hat{B}\\) whose sampling distribution has the following properties\n\\[\n\\mathbb{E}[\\hat{B}]=\\beta, \\hspace{4mm} var[\\hat{B}]=\\frac{\\sigma^2}{S_{xx}}\n\\]\nAnd the estimate of the error variance\n\\[\\begin{align*}\n\\hat{\\sigma}^2 & =\\frac{1}{n-2}\\sum (y_i -\\hat{y_i})^2\\\\\n& = \\frac{1}{n-2} \\left(S_{yy}-\\frac{S_{xy}^2}{S_{xx}}\\right)\n\\end{align*}\\]\n\n\nGoodness of fit\nTo better understand the goodness of fit of the model for the data at hand, we can study the total variation in the responses, as given by\n\\[\nS_{yy} = \\sum (y_i-\\bar{y})^2\n\\]\nLet’s see how:\n\\[\\begin{align*}\ny_i - \\bar{y} &= (y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y}) \\\\\n\\implies (y_i - \\bar{y})^2 & = \\left((y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y})\\right)^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2(y_i - \\hat{y_i})(\\hat{y_i}-\\bar{y})+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 [y_i -(\\hat{\\alpha}+\\hat{\\beta}x_i)][\\hat{\\alpha}+\\hat{\\beta}x_i-(\\hat{\\alpha}+\\hat{\\beta}\\bar{x})]+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\sum\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left[\\sum x_iy_i-\\bar{x}\\sum y_i -\\hat{\\alpha}\\sum x_i + n\\hat{\\alpha}\\bar{x}-\\hat{\\beta}\\sum x_i^2\\right.\\\\\n&\\left.\\hspace{4mm}+\\hat{\\beta}\\bar{x}\\sum x_i\\right]+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(\\sum x_iy_i-n\\bar{x}\\bar{y}\\right)-2\\hat{\\beta}^2\\left(\\sum x_i^2 - n\\bar{x}^2\\right)+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}S_{xy}-2\\hat{\\beta}^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\frac{S_{xy}}{S_{xx}}S_{xy}-2\\left(\\frac{S_{xy}}{S_{xx}}\\right)^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 +\\sum(\\hat{y_i}-\\bar{y})^2\\\\\nSS_{TOT} & = SS_{RES}+ SS_{REG}\n\\end{align*}\\]\nIn the case that the data are “close” to a line ( \\(|r|\\) high- a strong linear relationship) the model fits well, the fitted responses (the values on the fitted line) are close to the observed responses, and so \\(SS_{REG}\\) is relatively high with \\(SS_{RES}\\) relatively low.\nIn the case that the data are not “close” to a line ( \\(|r|\\) low - a weak linear relationship) the model does not fit so well, the fitted responses are not so close to the observed responses, and so \\(SS_{REG}\\) is relatively low and \\(SS_{RES}\\) relatively high.\nThe proportion of the total variability of the responses “explained” by a model is called the coefficient of determination, denoted \\(R^2\\) .\n\\[\nR^2 = \\frac{SS_{REG}}{SS_{TOT}} =\\frac{S_{xy}^2}{S_{xx}S_{yy}}\n\\]\nwhich takes value between 0 to 1, inclusive. The higher \\(R^2\\), the better fitting.\nFor our data, we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 32.47\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=6.97, \\hspace{4mm} S_{xy}=7.13\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{7.13}{8.53} = 0.836\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 3.247 - 0.836 \\times 3.566 = 0.266\n\\end{align*}\\]\nTherefore, the fitted line would be \\(\\hat{y}=0.266 + 0.836x\\). Now we see the other metrics\n\\[\\begin{align*}\nSS_{TOT} &= 6.97 \\\\\nSS_{REG} & = \\frac{S_{yy}^2}{S_{xx}} = \\frac{6.97^2}{8.53}=5.695\\\\\nSS_{RES} & = 6.97 - 5.695 = 1.275 \\\\\n\\implies \\hat{\\sigma}^2 & = \\frac{1.275}{8}=0.1594\\\\\nR^2 & = \\frac{5.695}{6.97}=0.817\n\\end{align*}\\]\n\n\nCode\n# Parameters for the line\nalpha = 0.266  \nbeta = 0.836   \n\n# Line values\nline_x = np.linspace(min(df.StockPrice), max(df.StockPrice), 100)  \nline_y = alpha + beta * line_x             \n\n# Plot\nplt.scatter(df.StockPrice, df.Portfolio, color='blue', label='Data Points')\nplt.plot(line_x, line_y, color='red', label=f'Line: y = {alpha} + {beta}x')\n\n# Labels and title\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.title('Scatter Plot with Regression Line')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nInference on \\(\\beta\\)\nWe can rewrite \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\), as\n\\[\\begin{align*}\n\\hat{\\beta}&= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i-\\bar{y}\\sum (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x}y_i)-\\bar{y}\\left(\\sum x_i -n\\bar{x}\\right)}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i}{S_{xx}}\n\\end{align*}\\]\nNow we recall that \\(\\hat{B}\\) is the random variable that has \\(\\hat{\\beta}\\) as its realization. Therefore, \\(\\hat{B}=\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\). We also recall that \\(\\mathbb{E}(Y_i)=\\alpha +\\beta x\\). Putting these together we obtain,\n\\[\\begin{align*}\n\\mathbb{E}[\\hat{B}] &= \\mathbb{E}\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right] = \\frac{\\sum (x_i -\\bar{x})\\mathbb{E}[Y_i]}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})(\\alpha + \\beta x_i)}{S_{xx}}\\\\\n& = \\frac{\\alpha \\sum (x_i-\\bar{x})+\\beta \\sum x_i (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\alpha \\left(\\sum x_i -n\\bar{x}\\right)+\\beta \\left(\\sum x_i^2-\\bar{x}\\sum x_i\\right)}{S_{xx}} \\\\\n& = \\frac{\\alpha (n\\bar{x}-n\\bar{x})+\\beta\\left(\\sum x_i^2-n\\bar{x}^2\\right)}{S_{xx}}\\\\\n& = \\frac{0+\\beta S_{xx}}{S_{xx}} = \\beta\n\\end{align*}\\]\nNow the fact that \\(Y_i'\\)s are uncorrelated. Therefore, \\(var\\left(\\sum (Y_i)\\right)=\\sum var(Y_i)\\) and we have \\(var(Y_i)=\\sigma^2\\). Therefore,\n\\[\\begin{align*}\nvar[\\hat{B}]& = var\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right]= \\frac{\\sum (x_i-\\bar{x})^2var[Y_i]}{S_{xx}^2}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})^2\\sigma^2}{S_{xx}^2} = \\frac{\\sigma^2}{S_{xx}^2}\\sum (x_i-\\bar{x})^2 = \\frac{\\sigma^2}{S_{xx}^2}S_{xx}\\\\\n& = \\frac{\\sigma^2}{S_{xx}}\n\\end{align*}\\]\nSince \\(\\mathbb{E}(\\hat{\\beta})=\\beta\\) and \\(var(\\hat{\\beta})=\\frac{\\sigma^2}{S_{xx}}\\) so\n\\[\nM = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\sigma^2}{S_{xx}}}}\\sim N(0,1)\n\\]\nand the observed variance \\(\\hat{\\sigma}^2\\) has the property\n\\[\nN = \\frac{(n-2)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi_{n-2}^2\n\\]\nSince \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent, it follows that\n\\[\n\\frac{M}{\\sqrt{\\frac{N}{n-2}}} \\sim t_{n-2}\n\\]\nIn other words: \\[\n\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})} = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\n\\]\nNow the big question is what’s the use of this mathematical jargon that we have learned so far? Let’s use our regression problem on stock data to explain.\n\\(H_0: \\beta =0\\), there is no linear relationship\nvs\n\\(H_1: \\beta&gt; 0\\), there is a linear relationship\nBased on our data we have \\(\\hat{\\beta} = 0.836\\) and \\(\\hat{\\sigma}^2=0.1594\\), and \\(S_{xx}=8.53\\). Therefore, under \\(H_0\\), the test statistic\n\\[\n\\frac{\\hat{\\beta}-0}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\text{ has a } t_{10-2} \\text{ or } t_8 \\text{ distribution}\n\\]\nBut the observed value of this statistic \\[\n\\frac{0.836-0}{\\sqrt{0.1594/8.53}}=6.1156\n\\]\nwhich is way higher than the critical value at \\(5\\%\\) significance level.\n\n\nCode\nfrom scipy.stats import t\n\n# Parameters\ndf = 8  # Degrees of freedom\nalpha = 0.05  # Upper tail probability\nt_critical = t.ppf(1 - alpha, df)  # Critical t-value at the 95th percentile\n\n# Generate x values for the t-distribution\nx = np.linspace(-4, 4, 500)\ny = t.pdf(x, df)\n\n# Plot the t-distribution\nplt.plot(x, y, label=f't_{df} Distribution', color='blue')\nplt.fill_between(x, y, where=(x &gt;= t_critical), color='red', alpha=0.5, label=f'Upper {alpha*100}% Area')\n\n# Annotate the critical t-value on the x-axis\nplt.axvline(t_critical, ymin=0.02, ymax=0.30,color='red', linestyle='--', label=f'Critical t-value = {t_critical:.2f}')\nplt.text(t_critical, -0.02, f'{t_critical:.2f}', color='red', ha='center', va='top')\n\n# Add a horizontal line at y = 0\nplt.axhline(0, color='black', linestyle='-', linewidth=0.8)\n\n# Labels, title, and legend\nplt.title(f\"t-Distribution with {df} Degrees of Freedom\")\nplt.xlabel(\"t\")\nplt.ylabel(\"Density\")\nplt.legend()\n\n# Adjust plot limits\n\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, we reject the null hypothesis \\(H_0\\) at the \\(5\\%\\) level and conclude that there is a very strong evidence that \\(\\beta&gt;0\\), i.e., the portfolio value is increasing over stock price.\nAlternatively, let’s put our analysis in a different approach. We claim that\n\\(H_0: \\beta=1\\), there is a linear relationship\nvs\n\\(H_1: \\beta \\ne 1\\)\nIn this case,\n\\[\nse(\\hat{\\beta}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{0.1594}{8.53}} =0.1367\n\\]\nTherefore, the \\(95\\%\\) confidence interval for \\(\\beta\\) is\n\\[\n\\hat{\\beta} \\pm \\left\\{t_{0.025,8}\\times se(\\hat{\\beta})\\right\\}=0.836 \\pm 2.306\\times 0.1367 = (0.5207,1.1512)\n\\]\nThe \\(95\\%\\) two-sided confidence interval contains the value \\(1\\), so the two-sided test conducted at \\(5\\%\\) level results in \\(H_0\\) being accepted.\n\n\nMean Response and Individual Response\n\nMean Response\n\nIf \\(\\mu_0\\) is the expected (mean) response for a value \\(x_0\\) of the predictor variable, that is \\(\\mu_0 = \\mathbb{E}[Y|x_0]=\\alpha +\\beta x_0\\), then \\(\\mu_0\\) is an unbiased estimator given by\n\n\\[\n\\hat{\\mu}_0 = \\hat{\\alpha}+\\hat{\\beta} x_0\n\\]\nand the variance of the estimator is given by\n\\[\nvar(\\hat{\\mu}_0) = \\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nTherefore,\n\\[\n\\frac{\\hat{\\mu}_0-\\mu_0}{se[\\hat{\\mu}_0] }= \\frac{\\hat{\\mu}_0-\\mu_0}{\\sqrt{\\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\n\nIndividual Response\nThe actual estimate of an individual response \\[\n\\hat{y}_0 = \\hat{\\alpha} +\\hat{\\beta}x_0\n\\]\n\nHowever, the uncertainty associated with this estimator, as indicated by its variance, is higher compared to the mean estimator because it relies on the value of an individual response \\(y_0\\) rather than the more stable mean. To account for the additional variability of an individual response relative to the mean, an extra term, \\(\\sigma^2\\), must be included in the variance expression for the estimator of a mean response.\n\n\\[\nvar[\\hat{y}_0] = \\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nThus,\n\\[\n\\frac{\\hat{y}-y_0}{se[\\hat{y}_0]}=\\frac{\\hat{y}-y_0}{\\sqrt{\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\nLet’s put this two idea through our example. If we want to find a \\(95\\%\\) confidence interval or the expected portfolio value on stock price of say, 360. In that case,\n\n\\[\n\\text{Estimate of the expected portfolio value} = 0.266+0.836\\times 3.6 = 3.276\n\\]\nand\n\\[\n\\text{se}[\\text{Estimate}] = \\sqrt{\\left(\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}=0.1263\n\\]\nSo, the \\(95\\%\\) CI\n\\[\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) = 3.276 \\pm 2.306\\times 0.1263 = (2.985,3.567)\n\\]\nThat is for a stock price of \\(\\$360\\), the expected portfolio value would be in the range of \\((\\$298.50,\\$356.70)\\)\nSimilarly, the \\(95\\%\\) CI for the predicted actual portfolio value\n\\[\\begin{align*}\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) &= 3.276 \\pm 2.306\\sqrt{\\left(1+\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}\\\\\n& = (2.3103,4.2417)\n\\end{align*}\\]\nor \\((\\$231.03,\\$424.17)\\)\n\n\n\nModel Accuracy\nThe residual from the fit at \\(x_i\\) is the estimated error which is defined by \\[\n\\hat{\\epsilon}_i = y_i - \\hat{y}_i\n\\]\n\nScatter plots of residuals versus the explanatory variable (or the fitted response values) are particularly insightful. A lack of random scatter in the residuals, such as the presence of a discernible pattern, indicates potential shortcomings in the model.\n\n\n\nCode\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\nx = df.StockPrice.values\ny = df.Portfolio.values \n\ny_hat = [0.266+0.836*i for i in x]\nplt.scatter(x, y-y_hat)\nplt.axhline(0)\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn this plot, we can see that the residuals tend to increase as \\(x\\) increases, indicates that the error variance is not bounded, but increasing with \\(x\\). So, the model is not the best one. A transformation of the responses may stabilize the error variance.   In certain case, for some growth models, the appropriate model is that the expected response is related to the exploratory variable through an exponential relationship, i.e.,\n\n\\[\\begin{align*}\n\\mathbb{E}[Y_i|X=x_i] &= \\alpha e^{\\beta x_i}\\\\\n\\implies z_i = \\log y_i & = \\eta + \\beta x_i + \\epsilon_i; \\hspace{4mm}\\text{where }\\eta=\\log \\alpha\n\\end{align*}\\]\n\n\nCode\nx = df.StockPrice.values\ny = np.log(df.Portfolio.values)\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 11.43\nSₓₓ: 8.53\nSᵧᵧ: 0.70\nSₓᵧ: 2.29\n \nr : 0.94\n\n\nNow we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 11.43\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=0.70, \\hspace{4mm} S_{xy}=2.29\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{2.29}{8.53} = 0.268\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 1.143 - 0.268 \\times 3.566 = 0.1873\n\\end{align*}\\]\n\n\nCode\nimport numpy as np\nz_hat = [np.log(0.1873)+0.268*i for i in x]\nz = np.log(y)\nplt.scatter(x, z-z_hat)\nplt.axhline(np.mean(z-z_hat))\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow the residuals look good, that is no special pattern or increasing the error variance.\nThanks for reading."
  },
  {
    "objectID": "posts/corrandreg/index.html#references",
    "href": "posts/corrandreg/index.html#references",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "References",
    "text": "References\n\nMontgomery, D. C., & Runger, G. C. (2014). Applied Statistics and Probability for Engineers. Wiley.\n\nCasella, G., & Berger, R. L. (2002). Statistical Inference. Duxbury.\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Routledge.\n\nSeber, G. A. F., & Lee, A. J. (2003). Linear Regression Analysis. Wiley.\nNeter, J., Kutner, M. H., Nachtsheim, C. J., & Wasserman, W. (1996). Applied Linear Statistical Models. Irwin.\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\nWeisberg, S. (2005). Applied Linear Regression. Wiley.\n\nBivariate Normal Distribution Explanation:\n\nRice, J. A. (2006). Mathematical Statistics and Data Analysis. Thomson Brooks/Cole.\n\nA detailed exploration of the bivariate normal distribution and its properties.\n\nFisher’s Transformation of Correlation Coefficients:\n\nFisher, R. A. (1921). On the probable error of a coefficient of correlation. Metron, 1, 3-32.\n\nThe foundational paper describing Fisher’s transformation and its use in hypothesis testing.\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  }
]