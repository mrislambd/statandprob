[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic Statistics and Probability",
    "section": "",
    "text": "Bayesian Inference in Machine Learning: Part 1\n\n\n\n\n\n\n\n\nSunday, July 28, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation, Bivariate, and Regression Analysis\n\n\n\n\n\n\n\n\nWednesday, December 18, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nReview probabilities\n\n\n\n\n\n\n\n\nThursday, August 22, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nSome Key Statistical Concepts for Interview Prep\n\n\n\n\n\n\n\n\nThursday, September 5, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Law of Large Numbers and Central Limit Theorem\n\n\n\n\n\n\n\n\nSaturday, March 15, 2025\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html#binary-search",
    "href": "index.html#binary-search",
    "title": "Basic Statistics and Probability",
    "section": "",
    "text": "Problem 1: Leetcode 69: Sqrt(x)\n\n\n\n\n\nGiven a non-negative integer, \\(x\\), return the square root of \\(x\\) rounded down to the nearest integer. The returned integer should be non-negative as well.\nYou may not use any built-in exponent function. For example, x**0.5 in python.\nExample:\nInput: x=4\nOutput: 2  \n\nInput: x=8\nOutput: 2\nExplanation: Square root of 4 is 2 and square root of 8 is 2.8284. But we need to round down to any fraction. Therefore, the square root of 8 is also 2.\nSolution:\nThe square root of any number \\(x\\ge 0\\) is less than or equal to \\(x\\). The brute force solution to this would be \\(\\mathcal{O}(\\sqrt{n})\\). Because, say \\(x=8\\), then\nfor \\(i=1\\) to 8: \\[\\begin{align*}\n  1^2 & = 1 &lt;8 \\\\\n  2^2 & = 4 &lt;8\\\\\n  3^2 & = 9 &gt;8\n\\end{align*}\\]\n\nIn contrast, if we explore binary search then the time complexity reduces to \\(\\mathcal{O}(\\log{n})\\). Say the square root is \\(s\\) which is the middle value in the range of 1 to \\(x\\). Then if \\(s^2&gt;x\\), we search for the root in the left half. Otherwise, if \\(s^2&lt;x\\) then we search the right side. However, when \\(s^2&lt;x\\), then \\(s\\) is a possible candidate for the square root.\nAlgorithm:\n\nset left value \\(l= 0\\), right value \\(r= x\\)\n\nCompute the middle value \\(m=l+(r-l)/2\\)\n\nIf \\(m^2 &gt; x\\) then search the left side: set \\(r=m-1\\)\n\nIf \\(m^2 &lt; x\\) then search the right side: set \\(l=m+1\\)\n\n\ndef square_root(x):\n  l, r = 0, x \n  sq = 0\n  while l&lt;=r:\n    m = l + (r-l)//2 \n    if m**2 &gt; x:\n      r= m-1\n    elif m**2 &lt; x:\n      l = m+1 \n      sq = m\n    else:\n      return m  \n  return sq \n      \nprint(square_root(6))\n\n2"
  },
  {
    "objectID": "index.html#array",
    "href": "index.html#array",
    "title": "Basic Statistics and Probability",
    "section": "Array",
    "text": "Array\n\n\n\n\n\n\nProblem 1: Intersection of two sets\n\n\n\n\n\nSay, we are given two sets \\(A=[2,3,5,6,8]\\) and \\(B=[4,6,8]\\). We want to find the intersection of the elements in these sets.\ndef intersection_of_two_sets(A,B):\n  set_a = set(A)\n  return [b for b in B if b in set_a]\n\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n[6, 8]\n\n\n\n\n\n\n\n\n\nProblem 2: Histogram from a given array and bin number\n\n\n\n\n\nSay, we are given an array \\(A=[1,2,2,3,3,3,4,4,4,4,5,5,5,5,5]\\) and number of bins. We want to\ndef generate_histogram(A, num_bins):\n  min_value = min(A)\n  max_value = max(A)\n  bin_width = (max_value-min_value)/num_bins"
  },
  {
    "objectID": "index.html#string",
    "href": "index.html#string",
    "title": "Basic Statistics and Probability",
    "section": "String",
    "text": "String\n\n\n\n\n\n\nProblem 1: Leetcode 242: Valid Anagram\n\n\n\n\n\nGiven two strings s and t, return true if t is an anagram of s, and false otherwise\nExample: Input: s=\"anagram\", t=\"nagaram\" Output: true\nSince the word anagram has 3 a’s, 1 n, 1 g, 1 r, and 1 m and nagaram has exactly the same number of the same alphabets, therefore they are anagram of each other.\nExample: Input: s=\"rat\", t=\"cat\" Output: false\nSince the word rat has 1 r, 1 a, and 1 t but cat has 2 elements same as rat but one element different. Therefore the answer is false.\nNote that, they both have the same length.\nConstraints:\n\n\\(1\\le s\\).length, \\(t\\).length \\(\\le 5\\times 10^4\\)\n\ns and t consists of lowercase English letters\n\nSolution:\nWe can use hasmap to solve this problem. Basically, we will create two hasmaps for two words and match the keys and values of the hasmaps. If they are equal then it’s an anagram, otherwise not.\n\ndef isAnagram(s,t):\n  if len(s) != len(t):\n    return False \n  \n  hash_s, hash_t = {}, {}\n  for i in range(len(s)):\n    hash_s[s[i]] = 1 + hash_s.get(s[i],0)  # get function collects the key and values.\n    hash_t[t[i]] = 1 + hash_t.get(t[i],0)  # if there's no key, 0 is the default value\n  \n  for c in hash_s:\n    if hash_s[c] != hash_t.get(c,0):       # Here, get function ensures there is no \n      return False                         # key error\n  return True\n\ns = \"anagram\"\nt = \"nagaram\"\n\nprint(isAnagram(s,t))\n\nu = \"rat\"\n\nprint(isAnagram(s,u))\n\nTrue\nFalse\n\n\nTime and Space Complexity:\nTime complexity \\(\\mathcal{O}(m+n)\\) where \\(m\\) and \\(n\\) are the length of s and t and memory complexity is the same \\(\\mathcal{O}(m+n)\\)\nOptimization: Can we solve the problem in \\(\\mathcal{O}(1)\\)? If we assume sort doesn’t require extra space, then\n\ndef isAnagram(s,t):\n  return sorted(s)== sorted(t)\n\ns= \"anagram\"\nt= \"nagaram\"\n\nprint(isAnagram(s,t))\n\nTrue"
  },
  {
    "objectID": "index.html#reference",
    "href": "index.html#reference",
    "title": "Basic Statistics and Probability",
    "section": "Reference",
    "text": "Reference\nAll my solutions here are based on the solutions found from NeetCode."
  },
  {
    "objectID": "posts/lln/index.html",
    "href": "posts/lln/index.html",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "Statistical inference is a fundamental part of data science and machine learning. Two of the most important theorems in probability theory that form the backbone of many statistical methods are the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT). It’s not uncommon to mix-up with these two concepts often."
  },
  {
    "objectID": "posts/corrandreg/index.html",
    "href": "posts/corrandreg/index.html",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "posts/corrandreg/index.html#introduction",
    "href": "posts/corrandreg/index.html#introduction",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "posts/corrandreg/index.html#correlation-analysis",
    "href": "posts/corrandreg/index.html#correlation-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nTo better explain, we will use the following hypothetical stock data of 10 companies with stock price and their corresponding proportion in the portfolio.\n\n\nCode\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\n\ndf.set_index('Stock', inplace=True)\n\ndf.T\n\n\n\n\n\n\n\n\nStock\nApple\nCiti\nMS\nWF\nGS\nGoogle\nAmazon\nTesla\nToyota\nSPY\n\n\n\n\nStockPrice\n2.11\n2.42\n2.52\n3.21\n3.62\n3.86\n4.13\n4.27\n4.51\n5.01\n\n\nPortfolio\n2.12\n2.16\n2.51\n2.65\n3.62\n3.15\n4.32\n3.31\n4.18\n4.45\n\n\n\n\n\n\n\nThe scatterplot of the data looks like this\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport matplotlib.pyplot as plt\nplt.scatter(df.StockPrice, df.Portfolio, color='red')\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the graph that there appears to be a linear relationship between the \\(x\\) and \\(y\\) values in this case. To find the relationship mathematically we define the followings\n\\[\\begin{align*}\nS_{xx}& = \\sum (x_i-\\bar{x})^2 = \\sum (x_i^2-2x_i\\bar{x}+\\bar{x}^2)\\\\\n& = \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2= \\sum x_i^2 - 2\\bar{x} n \\bar{x} + n \\bar{x}^2 = \\sum x_i ^2 - n \\bar{x}^2\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\nS_{yy}& = \\sum (y_i-\\bar{y})^2=\\sum y_i ^2 - n \\bar{y}^2\\\\\nS_{xy} & = \\sum (x_i-\\bar{x})^2 \\sum (y_i-\\bar{y})^2 = \\sum x_iy_i -n \\bar{xy}\n\\end{align*}\\]\nThe sample correlation coefficient \\(r\\) is then given as\n\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} = \\frac{\\sum x_i ^2 - n \\bar{x}^2}{\\sqrt{\\left(\\sum x_i ^2 - n \\bar{x}^2\\right)\\left(\\sum y_i ^2 - n \\bar{y}^2\\right)}}\n\\]\nYou may have seen a different formula to calculate this quantity which often looks a bit different\n\\[\n\\rho = Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{var(X)var(Y)}}\n\\]\nThe sample correlation coefficient, \\(r\\), is an estimator of the population correlation coefficient, \\(\\rho\\), in the same way as \\(\\bar{X}\\) is an estimator of \\(\\mu\\) or \\(S^2\\) is an estimator of \\(\\sigma^2\\) . Now the question is what does this \\(r\\) values mean?\n\n\n\n\n\n\n\nValue\nMeaning\n\n\n\n\n\\(r=1\\)\nThe two variables move together in the same direction in a perfect linear relationship.\n\n\n\\(0 &lt; r &lt; 1\\)\nThe two variables tend to move together in the same direction but there is NOT a direct relationship.\n\n\n\\(r= 0\\)\nThe two variables can move in either direction and show no linear relationship.\n\n\n\\(-1 &lt; r &lt; 0\\)\nThe two variables tend to move together in opposite directions but there is not a direct relationship.\n\n\n\\(r =-1\\)\nThe two variables move together in opposite directions in a perfect linear relationship.\n\n\n\nLet’s calculate the correlation of our stock data.\n\n\nCode\nimport math\nx = df.StockPrice.values\ny = df.Portfolio.values\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 32.47\nSₓₓ: 8.53\nSᵧᵧ: 6.97\nSₓᵧ: 7.13\n \nr : 0.92"
  },
  {
    "objectID": "posts/corrandreg/index.html#bivariate-analysis",
    "href": "posts/corrandreg/index.html#bivariate-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\nThe joint probability density function for \\(X\\) and \\(Y\\) in the bivariate normal distribution is given by:\n\\[\nf_{X,Y}(x, y) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}}\n\\exp\\left( -\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho\\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] \\right)\n\\]\n\nWhen \\(|\\rho| = 1\\), the denominator \\(\\sqrt{1-\\rho^2}\\) in the PDF becomes zero, which might appear problematic. However, what happens in this case is that the joint distribution degenerates into a one-dimensional structure (a line) rather than being a two-dimensional probability density.\n\nTo see why, consider the quadratic term inside the exponential:\n\\[\nQ = \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho \\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2}\n\\]\nWhen \\(|\\rho| = 1\\), this quadratic expression simplifies, as shown next.\nStart with the simplified \\(Q\\) when \\(|\\rho| = 1\\):\n\\[\\begin{align*}\nQ &= \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)^2 - 2\\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\cdot \\frac{y-\\mu_Y}{\\sigma_Y} \\right) + \\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\\\\\n&=\\left( \\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\n\\end{align*}\\]\nThis is a perfect square because the “cross term” cancels out all independent variability of \\(X\\) and \\(Y\\) when \\(|\\rho| = 1\\).\nFor the quadratic term \\(Q\\) to have any non-zero probability density (since it appears in the exponent of the PDF), it must be equal to zero: \\[\n\\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} = 0\n\\]\nRearranging this equation: \\[\n\\frac{y-\\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nMultiply through by \\(\\sigma_Y\\): \\[\ny - \\mu_Y = \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\]\nThus:\n\\[\\begin{align*}\n\\mathbb{E}(Y| X=x)&= \\mu_Y + \\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X)\\\\\n& = \\mu_Y + \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\end{align*}\\]\nThis is the equation of a straight line in the \\((X, Y)\\)-plane. The slope of the line is \\(\\rho \\frac{\\sigma_Y}{\\sigma_X}\\), and the line passes through the point \\((\\mu_X, \\mu_Y)\\). When \\(|\\rho| = 1\\), all the joint probability mass collapses onto this line, meaning \\(X\\) and \\(Y\\) are perfectly linearly dependent.\n\n\nCode\nimport numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the bivariate normal PDF\ndef bivariate_normal_pdf(x, y, mu_x, mu_y, sigma_x, sigma_y, rho):\n    z = (\n        ((x - mu_x) ** 2) / sigma_x**2\n        - 2 * rho * (x - mu_x) * (y - mu_y) / (sigma_x * sigma_y)\n        + ((y - mu_y) ** 2) / sigma_y**2\n    )\n    denominator = 2 * np.pi * sigma_x * sigma_y * np.sqrt(1 - rho**2)\n    return np.exp(-z / (2 * (1 - rho**2))) / denominator\n\n# Parameters\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Function to plot the bivariate normal distribution and a line for rho = 1 or -1\ndef plot_bivariate_and_line_side_by_side(rho1, rho2):\n    fig = plt.figure(figsize=(8, 4))\n\n    # Plot for the first rho\n    ax1 = fig.add_subplot(121, projection='3d')\n    if abs(rho1) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax1.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho1})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho1)\n        ax1.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax1.set_title(f'Bivariate Normal (ρ = {rho1:.2f})')\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('PDF')\n\n    # Plot for the second rho\n    ax2 = fig.add_subplot(122, projection='3d')\n    if abs(rho2) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax2.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho2})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho2)\n        ax2.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax2.set_title(f'Bivariate Normal (ρ = {rho2:.2f})')\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_zlabel('PDF')\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot examples side by side\nplot_bivariate_and_line_side_by_side(0.5, 1)  # Example with rho = 0.5 and rho = 1\n\n\n\n\n\n\n\n\n\n\n\\(t-\\)Statistic\nUnder the null hypothesis, where \\(H_0: \\rho =0, \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\) has a \\(t-\\) distribution with \\(\\nu=n-2\\) degree of freedom.\n\n\nFisher’s Transformation of \\(r\\)\nIf \\(W = \\frac{1}{2}\\ln{\\frac{1+r}{1-r}}=\\tanh^{-1}r\\), then \\(W\\) has approximately a normal distribution with mean \\(\\frac{1}{2}\\ln{\\frac{1+\\rho}{1-\\rho}}\\) and standard deviation \\(\\frac{1}{\\sqrt{n-3}}\\).\nFor our stock data:\nNull Hypothesis \\(H_0\\): There is no association between stock prices and the portfolio values, i.e., \\(\\rho =0\\)\nAlternative Hypothesis \\(H_1\\): There is some association between the stock price and portfolio values, i.e., \\(\\rho &gt; 0\\)\nIf \\(H_0\\) is true, then the test statistic \\(\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{0.92\\sqrt{8}}{\\sqrt{1-0.92^2}}=6.64\\) has a \\(t_8\\) distribution. The observed value \\(6.64\\) is much greater than the critical value of \\(t_8\\) at \\(0.5\\%\\) level which is \\(3.36\\).\nSo, we reject the null hypothesis \\(H_0\\) at the \\(0.5\\%\\) level and conclude that there is a very strong evidence that \\(\\rho&gt;0\\).\nAlternatively, if we want to use the Fisher’s test:\nIf \\(H_0\\) is true, then the test statistic \\(Z_r=\\tanh^{-1}r=\\tanh^{-1}(0.92)\\) has a \\(N\\left(0,\\frac{1}{7}\\right)\\) distribution.\nThe observed value of this statistic is \\(\\frac{1}{2}\\log{\\frac{1+0.92}{1-0.92}}=1.589\\), which corresponds to a value of \\(\\frac{1.589}{\\sqrt{\\frac{1}{7}}}=4.204\\) on the \\(N(0,1)\\) distribution. This is much greater than \\(3.090\\), the upper \\(0.1\\%\\) point of the standard normal distribution.\nSo, we reject \\(H_0\\) at the \\(0.1\\%\\) level and conclude that there is very strong evidence that \\(\\rho &gt; 0\\) ie that there is a positive linear correlation between the stock price and portfolio value."
  },
  {
    "objectID": "posts/corrandreg/index.html#regression-analysis",
    "href": "posts/corrandreg/index.html#regression-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Regression Analysis",
    "text": "Regression Analysis\nGiven a set of points \\((x_i,y_i)_{i=0}^{n}\\) for a simple linear regression of the form\n\\[\nY_i = \\alpha +\\beta x_i + \\epsilon_i; \\hspace{4mm} i=1,2,\\cdots,n\n\\]\nwith \\(\\mathbb{\\epsilon_i}=0\\) and \\(var[\\epsilon_i]=\\sigma^2\\).\n\nModel Fitting\nWe can estimate the parameters from the method of least squares but that’s not the goal in this case. Fitting the model involves finding \\(\\alpha\\) and \\(\\beta\\) and the estimating the variance \\(\\sigma^2\\).\n\\[\n\\hat{y} = \\hat{\\alpha}+\\hat{\\beta}x\n\\]\nwhere, \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\) and \\(\\hat{\\alpha} = \\bar{y}-\\hat{\\beta}\\bar{x}\\)\n\\(\\hat{\\beta}\\) is the observed value of a statistic \\(\\hat{B}\\) whose sampling distribution has the following properties\n\\[\n\\mathbb{E}[\\hat{B}]=\\beta, \\hspace{4mm} var[\\hat{B}]=\\frac{\\sigma^2}{S_{xx}}\n\\]\nAnd the estimate of the error variance\n\\[\\begin{align*}\n\\hat{\\sigma}^2 & =\\frac{1}{n-2}\\sum (y_i -\\hat{y_i})^2\\\\\n& = \\frac{1}{n-2} \\left(S_{yy}-\\frac{S_{xy}^2}{S_{xx}}\\right)\n\\end{align*}\\]\n\n\nGoodness of fit\nTo better understand the goodness of fit of the model for the data at hand, we can study the total variation in the responses, as given by\n\\[\nS_{yy} = \\sum (y_i-\\bar{y})^2\n\\]\nLet’s see how:\n\\[\\begin{align*}\ny_i - \\bar{y} &= (y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y}) \\\\\n\\implies (y_i - \\bar{y})^2 & = \\left((y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y})\\right)^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2(y_i - \\hat{y_i})(\\hat{y_i}-\\bar{y})+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 [y_i -(\\hat{\\alpha}+\\hat{\\beta}x_i)][\\hat{\\alpha}+\\hat{\\beta}x_i-(\\hat{\\alpha}+\\hat{\\beta}\\bar{x})]+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\sum\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left[\\sum x_iy_i-\\bar{x}\\sum y_i -\\hat{\\alpha}\\sum x_i + n\\hat{\\alpha}\\bar{x}-\\hat{\\beta}\\sum x_i^2\\right.\\\\\n&\\left.\\hspace{4mm}+\\hat{\\beta}\\bar{x}\\sum x_i\\right]+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(\\sum x_iy_i-n\\bar{x}\\bar{y}\\right)-2\\hat{\\beta}^2\\left(\\sum x_i^2 - n\\bar{x}^2\\right)+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}S_{xy}-2\\hat{\\beta}^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\frac{S_{xy}}{S_{xx}}S_{xy}-2\\left(\\frac{S_{xy}}{S_{xx}}\\right)^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 +\\sum(\\hat{y_i}-\\bar{y})^2\\\\\nSS_{TOT} & = SS_{RES}+ SS_{REG}\n\\end{align*}\\]\nIn the case that the data are “close” to a line ( \\(|r|\\) high- a strong linear relationship) the model fits well, the fitted responses (the values on the fitted line) are close to the observed responses, and so \\(SS_{REG}\\) is relatively high with \\(SS_{RES}\\) relatively low.\nIn the case that the data are not “close” to a line ( \\(|r|\\) low - a weak linear relationship) the model does not fit so well, the fitted responses are not so close to the observed responses, and so \\(SS_{REG}\\) is relatively low and \\(SS_{RES}\\) relatively high.\nThe proportion of the total variability of the responses “explained” by a model is called the coefficient of determination, denoted \\(R^2\\) .\n\\[\nR^2 = \\frac{SS_{REG}}{SS_{TOT}} =\\frac{S_{xy}^2}{S_{xx}S_{yy}}\n\\]\nwhich takes value between 0 to 1, inclusive. The higher \\(R^2\\), the better fitting.\nFor our data, we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 32.47\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=6.97, \\hspace{4mm} S_{xy}=7.13\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{7.13}{8.53} = 0.836\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 3.247 - 0.836 \\times 3.566 = 0.266\n\\end{align*}\\]\nTherefore, the fitted line would be \\(\\hat{y}=0.266 + 0.836x\\). Now we see the other metrics\n\\[\\begin{align*}\nSS_{TOT} &= 6.97 \\\\\nSS_{REG} & = \\frac{S_{yy}^2}{S_{xx}} = \\frac{6.97^2}{8.53}=5.695\\\\\nSS_{RES} & = 6.97 - 5.695 = 1.275 \\\\\n\\implies \\hat{\\sigma}^2 & = \\frac{1.275}{8}=0.1594\\\\\nR^2 & = \\frac{5.695}{6.97}=0.817\n\\end{align*}\\]\n\n\nCode\n# Parameters for the line\nalpha = 0.266  \nbeta = 0.836   \n\n# Line values\nline_x = np.linspace(min(df.StockPrice), max(df.StockPrice), 100)  \nline_y = alpha + beta * line_x             \n\n# Plot\nplt.scatter(df.StockPrice, df.Portfolio, color='blue', label='Data Points')\nplt.plot(line_x, line_y, color='red', label=f'Line: y = {alpha} + {beta}x')\n\n# Labels and title\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.title('Scatter Plot with Regression Line')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nInference on \\(\\beta\\)\nWe can rewrite \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\), as\n\\[\\begin{align*}\n\\hat{\\beta}&= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i-\\bar{y}\\sum (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x}y_i)-\\bar{y}\\left(\\sum x_i -n\\bar{x}\\right)}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i}{S_{xx}}\n\\end{align*}\\]\nNow we recall that \\(\\hat{B}\\) is the random variable that has \\(\\hat{\\beta}\\) as its realization. Therefore, \\(\\hat{B}=\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\). We also recall that \\(\\mathbb{E}(Y_i)=\\alpha +\\beta x\\). Putting these together we obtain,\n\\[\\begin{align*}\n\\mathbb{E}[\\hat{B}] &= \\mathbb{E}\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right] = \\frac{\\sum (x_i -\\bar{x})\\mathbb{E}[Y_i]}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})(\\alpha + \\beta x_i)}{S_{xx}}\\\\\n& = \\frac{\\alpha \\sum (x_i-\\bar{x})+\\beta \\sum x_i (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\alpha \\left(\\sum x_i -n\\bar{x}\\right)+\\beta \\left(\\sum x_i^2-\\bar{x}\\sum x_i\\right)}{S_{xx}} \\\\\n& = \\frac{\\alpha (n\\bar{x}-n\\bar{x})+\\beta\\left(\\sum x_i^2-n\\bar{x}^2\\right)}{S_{xx}}\\\\\n& = \\frac{0+\\beta S_{xx}}{S_{xx}} = \\beta\n\\end{align*}\\]\nNow the fact that \\(Y_i'\\)s are uncorrelated. Therefore, \\(var\\left(\\sum (Y_i)\\right)=\\sum var(Y_i)\\) and we have \\(var(Y_i)=\\sigma^2\\). Therefore,\n\\[\\begin{align*}\nvar[\\hat{B}]& = var\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right]= \\frac{\\sum (x_i-\\bar{x})^2var[Y_i]}{S_{xx}^2}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})^2\\sigma^2}{S_{xx}^2} = \\frac{\\sigma^2}{S_{xx}^2}\\sum (x_i-\\bar{x})^2 = \\frac{\\sigma^2}{S_{xx}^2}S_{xx}\\\\\n& = \\frac{\\sigma^2}{S_{xx}}\n\\end{align*}\\]\nSince \\(\\mathbb{E}(\\hat{\\beta})=\\beta\\) and \\(var(\\hat{\\beta})=\\frac{\\sigma^2}{S_{xx}}\\) so\n\\[\nM = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\sigma^2}{S_{xx}}}}\\sim N(0,1)\n\\]\nand the observed variance \\(\\hat{\\sigma}^2\\) has the property\n\\[\nN = \\frac{(n-2)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi_{n-2}^2\n\\]\nSince \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent, it follows that\n\\[\n\\frac{M}{\\sqrt{\\frac{N}{n-2}}} \\sim t_{n-2}\n\\]\nIn other words: \\[\n\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})} = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\n\\]\nNow the big question is what’s the use of this mathematical jargon that we have learned so far? Let’s use our regression problem on stock data to explain.\n\\(H_0: \\beta =0\\), there is no linear relationship\nvs\n\\(H_1: \\beta&gt; 0\\), there is a linear relationship\nBased on our data we have \\(\\hat{\\beta} = 0.836\\) and \\(\\hat{\\sigma}^2=0.1594\\), and \\(S_{xx}=8.53\\). Therefore, under \\(H_0\\), the test statistic\n\\[\n\\frac{\\hat{\\beta}-0}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\text{ has a } t_{10-2} \\text{ or } t_8 \\text{ distribution}\n\\]\nBut the observed value of this statistic \\[\n\\frac{0.836-0}{\\sqrt{0.1594/8.53}}=6.1156\n\\]\nwhich is way higher than the critical value at \\(5\\%\\) significance level.\n\n\nCode\nfrom scipy.stats import t\n\n# Parameters\ndf = 8  # Degrees of freedom\nalpha = 0.05  # Upper tail probability\nt_critical = t.ppf(1 - alpha, df)  # Critical t-value at the 95th percentile\n\n# Generate x values for the t-distribution\nx = np.linspace(-4, 4, 500)\ny = t.pdf(x, df)\n\n# Plot the t-distribution\nplt.plot(x, y, label=f't_{df} Distribution', color='blue')\nplt.fill_between(x, y, where=(x &gt;= t_critical), color='red', alpha=0.5, label=f'Upper {alpha*100}% Area')\n\n# Annotate the critical t-value on the x-axis\nplt.axvline(t_critical, ymin=0.02, ymax=0.30,color='red', linestyle='--', label=f'Critical t-value = {t_critical:.2f}')\nplt.text(t_critical, -0.02, f'{t_critical:.2f}', color='red', ha='center', va='top')\n\n# Add a horizontal line at y = 0\nplt.axhline(0, color='black', linestyle='-', linewidth=0.8)\n\n# Labels, title, and legend\nplt.title(f\"t-Distribution with {df} Degrees of Freedom\")\nplt.xlabel(\"t\")\nplt.ylabel(\"Density\")\nplt.legend()\n\n# Adjust plot limits\n\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, we reject the null hypothesis \\(H_0\\) at the \\(5\\%\\) level and conclude that there is a very strong evidence that \\(\\beta&gt;0\\), i.e., the portfolio value is increasing over stock price.\nAlternatively, let’s put our analysis in a different approach. We claim that\n\\(H_0: \\beta=1\\), there is a linear relationship\nvs\n\\(H_1: \\beta \\ne 1\\)\nIn this case,\n\\[\nse(\\hat{\\beta}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{0.1594}{8.53}} =0.1367\n\\]\nTherefore, the \\(95\\%\\) confidence interval for \\(\\beta\\) is\n\\[\n\\hat{\\beta} \\pm \\left\\{t_{0.025,8}\\times se(\\hat{\\beta})\\right\\}=0.836 \\pm 2.306\\times 0.1367 = (0.5207,1.1512)\n\\]\nThe \\(95\\%\\) two-sided confidence interval contains the value \\(1\\), so the two-sided test conducted at \\(5\\%\\) level results in \\(H_0\\) being accepted.\n\n\nMean Response and Individual Response\n\nMean Response\n\nIf \\(\\mu_0\\) is the expected (mean) response for a value \\(x_0\\) of the predictor variable, that is \\(\\mu_0 = \\mathbb{E}[Y|x_0]=\\alpha +\\beta x_0\\), then \\(\\mu_0\\) is an unbiased estimator given by\n\n\\[\n\\hat{\\mu}_0 = \\hat{\\alpha}+\\hat{\\beta} x_0\n\\]\nand the variance of the estimator is given by\n\\[\nvar(\\hat{\\mu}_0) = \\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nTherefore,\n\\[\n\\frac{\\hat{\\mu}_0-\\mu_0}{se[\\hat{\\mu}_0] }= \\frac{\\hat{\\mu}_0-\\mu_0}{\\sqrt{\\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\n\nIndividual Response\nThe actual estimate of an individual response \\[\n\\hat{y}_0 = \\hat{\\alpha} +\\hat{\\beta}x_0\n\\]\n\nHowever, the uncertainty associated with this estimator, as indicated by its variance, is higher compared to the mean estimator because it relies on the value of an individual response \\(y_0\\) rather than the more stable mean. To account for the additional variability of an individual response relative to the mean, an extra term, \\(\\sigma^2\\), must be included in the variance expression for the estimator of a mean response.\n\n\\[\nvar[\\hat{y}_0] = \\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nThus,\n\\[\n\\frac{\\hat{y}-y_0}{se[\\hat{y}_0]}=\\frac{\\hat{y}-y_0}{\\sqrt{\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\nLet’s put this two idea through our example. If we want to find a \\(95\\%\\) confidence interval or the expected portfolio value on stock price of say, 360. In that case,\n\n\\[\n\\text{Estimate of the expected portfolio value} = 0.266+0.836\\times 3.6 = 3.276\n\\]\nand\n\\[\n\\text{se}[\\text{Estimate}] = \\sqrt{\\left(\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}=0.1263\n\\]\nSo, the \\(95\\%\\) CI\n\\[\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) = 3.276 \\pm 2.306\\times 0.1263 = (2.985,3.567)\n\\]\nThat is for a stock price of \\(\\$360\\), the expected portfolio value would be in the range of \\((\\$298.50,\\$356.70)\\)\nSimilarly, the \\(95\\%\\) CI for the predicted actual portfolio value\n\\[\\begin{align*}\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) &= 3.276 \\pm 2.306\\sqrt{\\left(1+\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}\\\\\n& = (2.3103,4.2417)\n\\end{align*}\\]\nor \\((\\$231.03,\\$424.17)\\)\n\n\n\nModel Accuracy\nThe residual from the fit at \\(x_i\\) is the estimated error which is defined by \\[\n\\hat{\\epsilon}_i = y_i - \\hat{y}_i\n\\]\n\nScatter plots of residuals versus the explanatory variable (or the fitted response values) are particularly insightful. A lack of random scatter in the residuals, such as the presence of a discernible pattern, indicates potential shortcomings in the model.\n\n\n\nCode\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\nx = df.StockPrice.values\ny = df.Portfolio.values \n\ny_hat = [0.266+0.836*i for i in x]\nplt.scatter(x, y-y_hat)\nplt.axhline(0)\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn this plot, we can see that the residuals tend to increase as \\(x\\) increases, indicates that the error variance is not bounded, but increasing with \\(x\\). So, the model is not the best one. A transformation of the responses may stabilize the error variance.   In certain case, for some growth models, the appropriate model is that the expected response is related to the exploratory variable through an exponential relationship, i.e.,\n\n\\[\\begin{align*}\n\\mathbb{E}[Y_i|X=x_i] &= \\alpha e^{\\beta x_i}\\\\\n\\implies z_i = \\log y_i & = \\eta + \\beta x_i + \\epsilon_i; \\hspace{4mm}\\text{where }\\eta=\\log \\alpha\n\\end{align*}\\]\n\n\nCode\nx = df.StockPrice.values\ny = np.log(df.Portfolio.values)\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 11.43\nSₓₓ: 8.53\nSᵧᵧ: 0.70\nSₓᵧ: 2.29\n \nr : 0.94\n\n\nNow we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 11.43\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=0.70, \\hspace{4mm} S_{xy}=2.29\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{2.29}{8.53} = 0.268\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 1.143 - 0.268 \\times 3.566 = 0.1873\n\\end{align*}\\]\n\n\nCode\nimport numpy as np\nz_hat = [np.log(0.1873)+0.268*i for i in x]\nz = np.log(y)\nplt.scatter(x, z-z_hat)\nplt.axhline(np.mean(z-z_hat))\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow the residuals look good, that is no special pattern or increasing the error variance.\nThanks for reading."
  },
  {
    "objectID": "posts/corrandreg/index.html#references",
    "href": "posts/corrandreg/index.html#references",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "References",
    "text": "References\n\nMontgomery, D. C., & Runger, G. C. (2014). Applied Statistics and Probability for Engineers. Wiley.\n\nCasella, G., & Berger, R. L. (2002). Statistical Inference. Duxbury.\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Routledge.\n\nSeber, G. A. F., & Lee, A. J. (2003). Linear Regression Analysis. Wiley.\nNeter, J., Kutner, M. H., Nachtsheim, C. J., & Wasserman, W. (1996). Applied Linear Statistical Models. Irwin.\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\nWeisberg, S. (2005). Applied Linear Regression. Wiley.\n\nBivariate Normal Distribution Explanation:\n\nRice, J. A. (2006). Mathematical Statistics and Data Analysis. Thomson Brooks/Cole.\n\nA detailed exploration of the bivariate normal distribution and its properties.\n\nFisher’s Transformation of Correlation Coefficients:\n\nFisher, R. A. (1921). On the probable error of a coefficient of correlation. Metron, 1, 3-32.\n\nThe foundational paper describing Fisher’s transformation and its use in hypothesis testing.\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/bayesianinference/index.html",
    "href": "posts/bayesianinference/index.html",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?  To solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#introduction",
    "href": "posts/bayesianinference/index.html#introduction",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?  To solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "href": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "Why Bayesian Inference in Machine Learning?",
    "text": "Why Bayesian Inference in Machine Learning?\nBayesian inference plays a crucial role in machine learning, particularly in areas involving uncertainty and probabilistic reasoning. It allows us to incorporate prior knowledge and update beliefs based on new data, which is especially useful in the following applications:\n\nBayesian Networks\nBayesian networks are graphical models that represent the probabilistic relationships among a set of variables. Each node in the network represents a random variable, and the edges represent conditional dependencies. Bayesian networks are used for various tasks such as classification, prediction, and anomaly detection.\n\n\nBayesian Regression\nBayesian regression extends linear regression by incorporating prior distributions on the model parameters. This approach provides a probabilistic framework for regression analysis, allowing for uncertainty in the parameter estimates. The posterior distribution of the parameters is computed using Bayes’s theorem, and predictions are made by averaging over this distribution.\n\n\nSampling Methods\nIn Bayesian inference, exact computation of the posterior distribution is often intractable. Therefore, sampling methods such as Markov Chain Monte Carlo (MCMC) and Variational Inference are used to approximate the posterior distribution. These methods generate samples from the posterior distribution, allowing us to estimate various statistical properties and make inferences.\nMarkov Chain Monte Carlo (MCMC)\nMCMC methods generate a sequence of samples from the posterior distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. Common MCMC algorithms include the Underdamped and Overdamped Langevin dynamics, Metropolis-Hastings algorithm and the Gibbs sampler.\nExample: Metropolis-Hastings Algorithm\nConsider a posterior distribution \\(P(\\theta|D)\\) where \\(\\theta\\) represents the model parameters and \\(D\\) represents the data. The Metropolis-Hastings algorithm proceeds as follows:\n\nInitialize the parameters \\(\\theta_0\\).\nFor \\(t = 1\\) to \\(T\\):\n\nPropose a new state \\(\\theta'\\) from a proposal distribution \\(Q(\\theta'|\\theta_t)\\).\nCompute the acceptance ratio \\(\\alpha = \\frac{P(\\theta'|D) \\cdot Q(\\theta_t|\\theta')}{P(\\theta_t|D) \\cdot Q(\\theta'|\\theta_t)}\\).\nAccept the new state with probability \\(\\min(1, \\alpha)\\). If accepted, set \\(\\theta_{t+1} = \\theta'\\); otherwise, set \\(\\theta_{t+1} = \\theta_t\\).\n\n\nThe samples \\(\\theta_1, \\theta_2, \\ldots, \\theta_T\\) form a Markov chain whose stationary distribution is the posterior distribution \\(P(\\theta|D)\\).\n\n\nBayesian Inference in Neural Networks\nBayesian methods are also applied to neural networks, resulting in Bayesian Neural Networks (BNNs). BNNs incorporate uncertainty in the network weights by placing a prior distribution over them and using Bayes’s theorem to update this distribution based on the observed data. This allows BNNs to provide not only point estimates but also uncertainty estimates for their predictions.\nIn the next parts, we will talk about different applications of the Bayesian inferences, specifically, sampling problem using Langevin dynamics.\n\n\nReference\n\nPancake problems on mathstackexchance\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Review probabilities",
    "section": "",
    "text": "In today’s world, getting placement in data science world is highly challenging and competitive. It requires a lot of things including but not limited to basic knowledge in statistics, probability, machine learning, deep learning, and computer science. Even sometimes we face some basic problems from statistics and probability that we probably have solve long ago but forgot due to lack of practice or it’s taking longer due to rusty memory. Because, in master’s and Ph.D’s we focus on a very narrow topic and get our experties on those topics. So, it’s not a shame or humiliation if we can’t do a very simple problem even though we are capable of solving thousand time harder problems than that. It’s normal."
  },
  {
    "objectID": "posts/probability/index.html#introduction",
    "href": "posts/probability/index.html#introduction",
    "title": "Review probabilities",
    "section": "",
    "text": "In today’s world, getting placement in data science world is highly challenging and competitive. It requires a lot of things including but not limited to basic knowledge in statistics, probability, machine learning, deep learning, and computer science. Even sometimes we face some basic problems from statistics and probability that we probably have solve long ago but forgot due to lack of practice or it’s taking longer due to rusty memory. Because, in master’s and Ph.D’s we focus on a very narrow topic and get our experties on those topics. So, it’s not a shame or humiliation if we can’t do a very simple problem even though we are capable of solving thousand time harder problems than that. It’s normal."
  },
  {
    "objectID": "posts/probability/index.html#conditional-probabilities-bayess-theorem",
    "href": "posts/probability/index.html#conditional-probabilities-bayess-theorem",
    "title": "Review probabilities",
    "section": "Conditional Probabilities: Bayes’s Theorem",
    "text": "Conditional Probabilities: Bayes’s Theorem\n\nAssume two coins, one fair (i.e. equal chance of getting head and tail if tossed) and the other one is unfair and always gets head if tossed. If a coin is chosen at random and tossed six times and you get heads in all six tosses, what is the probability that you are tossing the unfair one?\nSolution:\nLet,\n\n\\(F\\) be the event the coin is fair, \\(F'\\) being the event of unfair coin and\n\n\\(H\\) be the event showing up head.\n\nWe neeed to find \\(\\mathbb{P}(F'|6H)\\), the probability that we are tossing the unfair \\(F'\\) coins given that we got 6 heads.\n\\[\\begin{align}\n     \\mathbb{P}(F'|6H)&=\\frac{\\mathbb{P}(6H|F')\\mathbb{P}(F')}{\\mathbb{P}(6H)}\n\\end{align}\\]\nHere,\n\n\\(\\mathbb{P}(F)=\\mathbb{P}(F')=\\frac{1}{2}\\), the probability of chosing a fair or unfair coin\n\\(\\mathbb{P}(6H|F)=\\left(\\frac{1}{2}\\right)^6\\), by the principle that flipping a fair coin 6 times are indpendent events, and thus the probability got multiplied\n\\(\\mathbb{P}(6H|F')=1\\), sure event, since unfair coin.\n\nSo, the total probability, \\[\\mathbb{P}(6H)=\\mathbb{P}(6H|F)\\mathbb{P}(F)+\\mathbb{P}(6H|F')\\mathbb{P}(F')=\\left(\\frac{1}{2}\\right)^6\\frac{1}{2}+1\\cdot \\frac{1}{2}\\]\nTherefore,\n\\[\\mathbb{P}(F'|6H)=\\frac{\\mathbb{P}(6H|F')\\mathbb{P}(F')}{\\mathbb{P}(6H)}=\\frac{1\\cdot\\frac{1}{2}}{\\left(\\frac{1}{2}\\right)^6\\frac{1}{2}+1\\cdot \\frac{1}{2}}\\]\nOne in thousand people\n\nYou may also like"
  },
  {
    "objectID": "posts/statisticaltalk/index.html",
    "href": "posts/statisticaltalk/index.html",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "",
    "text": "In the world of data analysis and machine learning, statistics plays a vital role in making sense of the data. Whether you’re estimating parameters, testing hypotheses, or understanding relationships between variables, statistical concepts guide how we interpret data. In this post, I want to summarise and collect some fundamental statistical ideas that are quite common and asked in many data science, machine learning, and quant interviews"
  },
  {
    "objectID": "posts/statisticaltalk/index.html#basic-statistical-terminologies",
    "href": "posts/statisticaltalk/index.html#basic-statistical-terminologies",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "Basic Statistical Terminologies",
    "text": "Basic Statistical Terminologies\n\nThe mean\nThe mean is one of the most basic statistical concepts and represents the average value of a dataset. It’s calculated by summing all the values in a dataset and then dividing by the number of observations.\nMathematically, for a set of discrete observations \\(x_1, x_2, ..., x_n\\), the mean \\(\\mu\\) or Expected Value is defined as:\n\\[\\begin{align*}\n\\mu &=  \\frac{1}{n} \\sum_{i=1}^{n} x_i\\\\\n\\implies \\mathbb{E}[X] &= \\sum_{i=1}^{n} x_i\\mathbb{P}(X=x_i)\n\\end{align*}\\]\nFor a continuous random variable \\(X\\), the mean\n\\[\n\\mu = \\mathbb{E}[X]=\\int_{-\\infty}^{\\infty}xf_X(x)dx\n\\]\n\nwhere, \\(\\mathbb{P}(X=x)\\) is the probability mass function (pmf) and \\(f_X(x)\\) is the probability density function (pdf) of the random variable \\(X\\), depending on whether it is discrete or contineous type. The mean helps describe the central tendency of data, but it can be sensitive to outliers.\n\n\n\nVariance\n\nVariance measures the spread or dispersion of a dataset relative to its mean. It tells us how far the individual data points are from the mean. A small variance indicates that data points are clustered closely around the mean, while a large variance means they are spread out.\n\nThe formula for variance \\(\\sigma^2\\) is:\n\\[\\begin{align*}\n    \\sigma^2=Var(X)&=\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\\\\\n    &=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^2\\right]\\\\\n    &=\\mathbb{E}\\left[(X^2-2X\\mathbb{E}[X]+(\\mathbb{E}[X])^2)\\right]\\\\\n    &=\\mathbb{E}[X^2]-2\\mathbb{E}[X]\\mathbb{E}[X]+(\\mathbb{E}[X])^2\\\\\n    &=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2\\\\\n\\end{align*}\\]\nHowever, the population and sample variance formula are slightly different. For discrete observations, the sample variance is given as\n\\[ s= \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nInstead of dividing by \\(n\\) we devide by \\(n-1\\) to have the sample variance unbiased and bigger than the population variance so that it contains the true population variance.\nExamples\n\nNormal Distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]}\\)\n\nStandard Normal Distribution with mean \\(0\\) and variance \\(1\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{x^2}{2}\\right]}\\)\n\nNow if \\(\\log X\\sim \\mathbfcal{N}(0,1)\\) then what is the distribution of \\(X\\)?\n\n\n\nCovariance\n\nCovariance measures how two variables move together. If the covariance is positive, the two variables tend to increase or decrease together. If negative, one variable tends to increase when the other decreases.\n\nThe formula for covariance between two variables \\(X\\) and \\(Y\\) is:\n\\[\n\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu_X)(Y_i - \\mu_Y)\n\\]\nHowever, covariance doesn’t indicate the strength of the relationship, which brings us to correlation.\n\n\nCorrelation\n\nCorrelation is a standardized measure of the relationship between two variables. It ranges from \\(-1\\) to \\(1\\), where \\(1\\) indicates a perfect positive relationship, \\(-1\\) a perfect negative relationship, and \\(0\\) no relationship.\n\nThe most common correlation metric is Pearson correlation, defined as:\n\\[\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nUnlike covariance, correlation gives a clearer picture of the strength and direction of a linear relationship between variables.\n\n\nP-Values and Hypothesis Testing\n\nP-values and hypothesis testing form the backbone of inferential statistics. Hypothesis testing is used to determine if a given assumption (the null hypothesis \\(H_0\\)) about a population parameter is true or not.\n\n\nThe null hypothesis \\(H_0\\) typically suggests no effect or no difference.\nThe alternative hypothesis \\(H_1\\) is the claim you want to test.\n\nThe p-value is the probability of observing a result as extreme as, or more extreme than, the one obtained, assuming the null hypothesis is true. A small p-value (usually less than 0.05) indicates that the null hypothesis is unlikely, and we may reject it in favor of the alternative hypothesis.\n\n\nMaximum Likelihood Estimation (MLE)\n\nMaximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model. The idea behind MLE is to find the parameter values that maximize the likelihood function, which represents the probability of observing the given data under a particular model.\n\nGiven a parameter \\(\\theta\\) and observed data \\(X\\), the likelihood function is:\n\\[\nL(\\theta | X) = P(X | \\theta)\n\\]\nMLE finds the parameter \\(\\hat{\\theta}\\) that maximizes this likelihood:\n\\[\n\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta | X)\n\\]\nMLE is widely used in statistical modeling, from simple linear regression to complex machine learning algorithms.\n\n\nMaximum A Posteriori (MAP)\nWhile MLE focuses on maximizing the likelihood, Maximum A Posteriori (MAP) estimation incorporates prior information about the parameters. MAP is rooted in Bayesian statistics, where the goal is to find the parameter that maximizes the posterior distribution.\nThe posterior is given by Bayes’ Theorem:\n\\[\nP(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\n\\]\nMAP finds the parameter \\(\\hat{\\theta}_{\\text{MAP}}\\) that maximizes the posterior probability:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(\\theta | X)\n\\]\nUnlike MLE, MAP estimation incorporates the prior distribution \\(P(\\theta)\\), making it more robust when prior knowledge is available\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/lln/index.html#the-law-of-large-numbers-lln",
    "href": "posts/lln/index.html#the-law-of-large-numbers-lln",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "The Law of Large Numbers (LLN)",
    "text": "The Law of Large Numbers (LLN)\n\nIntuition Behind LLN\n\nThe Law of Large Numbers states that as the sample size increases, the sample mean approaches the population mean. In other words, with more observations, the average of the sample becomes a better estimate of the true average of the population.\n\n\n\nMathematical Definition\nLet \\(X_1, X_2, X_3, \\dots, X_n\\) be a sequence of independent and identically distributed (i.i.d.) random variables with an expected value \\(\\mu\\). The sample mean is given by:\n\\[\n\\bar{X_n} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n\\] According to LLN: \\[\n\\bar{X_n} \\to \\mu \\quad \\text{as } n \\to \\infty\n\\]\nThis means that for a sufficiently large \\(n\\), \\(\\bar{X_n}\\) will be very close to \\(\\mu\\). That is\n\\[\n\\lim_{n\\to \\infty} \\bar{X}_n = \\lim_{n\\to \\infty} \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\mu\n\\]\n\n\nVisualization\nSay, we have a population of 150,000 male in a country called VSA (a hypothetical country)\n\n\nCode\nimport numpy as np\n\nheights = np.random.randint(low=90, high=190, size=150000)\nmean_height = np.mean(heights)\n\n\nand the true mean/average height of men is 139.43. We want to see how varying the sample size affect the sample mean and variances.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\nsample_sizes = [50, 100, 1000, 5000]\nnumber_of_studies = 10000\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(8.3, 5.5))\naxes = axes.ravel()\n\nfor i, n in enumerate(sample_sizes):\n    sample_means = np.zeros(number_of_studies)\n    for j in range(number_of_studies):\n        sample = np.random.choice(heights, size=n, replace=True)\n        sample_means[j] = np.mean(sample)\n    \n    # Plot histogram\n    axes[i].hist(sample_means, bins=35, density=True, alpha=0.7, color='blue')\n    ymin, ymax = axes[i].get_ylim()\n    axes[i].plot([mean_height, mean_height], [ymin, ymax], color='red', linestyle='-',linewidth=3, label='True Mean')\n    axes[i].set_title(f\"Sample Size: {n}\")\n    axes[i].set_xlabel(\"Sample Mean\")\n    axes[i].set_ylabel(\"Density\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.savefig('lln.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFrom the top-left figure, when the sample size is 50, the sample mean varies from 130 to 150 where the true population mean is 139.43, similarly in the 4th figure (bottom-left), when the sample size is 5000, the mean varies from 138 to 141. Thus, if we increase the sample size, sufficiently large, then the sample mean is approximately equal to the population mean. This is what LLN is!\n\n\n\nTypes of LLN\nThere are two forms of the Law of Large Numbers:\n\nWeak Law of Large Numbers (WLLN): Convergence of the sample mean to the population mean happens in probability.\nStrong Law of Large Numbers (SLLN): Convergence happens almost surely, meaning with probability 1.\n\n\n\nWhy is LLN Important?\nLLN justifies the use of sample means in estimation problems. For example, if we want to estimate the average income of a country, we don’t need to survey the entire population; we can take a large enough sample, and the sample mean will approximate the true mean."
  },
  {
    "objectID": "posts/lln/index.html#the-central-limit-theorem-clt",
    "href": "posts/lln/index.html#the-central-limit-theorem-clt",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "The Central Limit Theorem (CLT)",
    "text": "The Central Limit Theorem (CLT)\n\nIntuition Behind CLT\n\nWhile the Law of Large Numbers tells us that sample means converge to the population mean, the Central Limit Theorem goes further. It states that the distribution of the sample mean follows a normal distribution, regardless of the shape of the population distribution, provided the sample size is large enough.\n\n\n\nMathematical Definition\nLet \\(X_1, X_2, \\dots, X_n\\) be i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Define the sample mean: \\[\n\\bar{X_n} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n\\]\nThen, as \\(n\\) increases, the distribution of \\(\\bar{X_n}\\) approaches a normal distribution: \\[\n\\frac{\\bar{X_n} - \\mu}{\\sigma / \\sqrt{n}} \\approx N(0,1)\n\\]\nThis means that if we standardize \\(\\bar{X_n}\\), it follows a standard normal distribution (mean 0, variance 1) when \\(n\\) is large.\n\n\nVisualization\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define population distributions\ndef generate_population(dist_type, size=1000000):\n    if dist_type == \"uniform\":\n        return np.random.uniform(0, 100, size)\n    elif dist_type == \"exponential\":\n        return np.random.exponential(scale=10, size=size)\n    elif dist_type == \"binomial\":\n        return np.random.binomial(n=10, p=0.5, size=size)\n    else:\n        raise ValueError(\"Unknown distribution type\")\n\n# Function to simulate sample means\ndef sample_means_experiment(population, sample_size, num_samples=10000):\n    sample_means = np.zeros(num_samples)\n    for i in range(num_samples):\n        sample = np.random.choice(population, size=sample_size, replace=True)\n        sample_means[i] = np.mean(sample)\n    return sample_means\n\n# Define sample sizes\nsample_sizes = [5, 100, 1000]\n\n# Define distributions to test\ndistributions = [\"uniform\", \"exponential\", \"binomial\"]\n\n# Plot CLT effect for different distributions\nfig, axes = plt.subplots(len(distributions), len(sample_sizes), figsize=(8, 7.5))\nfig.suptitle(\"Central Limit Theorem: Sample Mean Distributions\", fontsize=16, fontweight='bold')\n\nfor i, dist_type in enumerate(distributions):\n    population = generate_population(dist_type)\n    true_mean = np.mean(population)\n    \n    for j, n in enumerate(sample_sizes):\n        sample_means = sample_means_experiment(population, n)\n\n        # Plot histogram of sample means\n        sns.histplot(sample_means, bins=20, kde=True, ax=axes[i, j], color='blue')\n        ymin, ymax = axes[i, j].get_ylim()\n        axes[i, j].plot([true_mean, true_mean], [ymin, ymax], 'r-', linewidth=2, label=\"True Mean\")\n        \n        axes[i, j].set_title(f\"{dist_type.capitalize()} Dist, n = {n}\")\n        axes[i, j].set_xlabel(\"Sample Mean\")\n        axes[i, j].set_ylabel(\"Density\")\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout for title\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhy is CLT Important?\n\nAllows Statistical Inference: Even if the population distribution is unknown or non-normal, we can still use normal-based statistical techniques when working with large samples.\nEnables Hypothesis Testing and Confidence Intervals: Many inferential statistics methods assume normality. CLT ensures that these methods work for large enough samples.\nMakes Sampling Practical: Without CLT, we would need to know the entire population distribution to make inferences.\n\n\n\nExample\n\nSuppose we have a population where the income distribution is highly skewed. If we take small samples, their distributions may also be skewed. However, as the sample size increases (e.g., \\(n &gt; 30\\)), the distribution of sample means will look more like a normal distribution, allowing us to apply normal-based statistical methods."
  },
  {
    "objectID": "posts/lln/index.html#relationship-between-lln-and-clt",
    "href": "posts/lln/index.html#relationship-between-lln-and-clt",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "Relationship Between LLN and CLT",
    "text": "Relationship Between LLN and CLT\nWhile both the Law of Large Numbers and the Central Limit Theorem deal with large samples, they serve different purposes:\n\nLLN guarantees that the sample mean converges to the true population mean as the sample size increases.\nCLT ensures that the distribution of the sample mean follows a normal distribution when the sample size is sufficiently large.\n\nIn short, LLN helps us estimate population parameters accurately, while CLT helps us conduct statistical inference using normal approximations."
  },
  {
    "objectID": "posts/lln/index.html#conclusion",
    "href": "posts/lln/index.html#conclusion",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe Law of Large Numbers and the Central Limit Theorem are two of the most fundamental theorems in probability and statistics. LLN reassures us that as we collect more data, our sample mean becomes a reliable estimate of the population mean. CLT, on the other hand, enables powerful statistical techniques by ensuring that sample means follow a normal distribution, even when the underlying population is not normal.  Understanding these concepts is essential for data science, as they form the basis for many machine learning and statistical inference methods. Whether you are estimating a population parameter, conducting hypothesis tests, or building predictive models, LLN and CLT provide the theoretical foundation for making reliable decisions based on data.\n\n\n\nFurther Reading\n\n“Introduction to Probability” by Joseph K. Blitzstein and Jessica Hwang\n“The Elements of Statistical Learning” by Hastie, Tibshirani, and Friedman\nAny introductory statistics textbook covering probability theory\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  }
]